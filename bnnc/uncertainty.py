from math import log

import numpy as np
import numpy.typing as npt

def global_uncertainty(p: npt.NDArray) -> float:
    """
    Calculates Global uncertainty (H, Predictive entropy)
    
    # Parameters:
    p: Predictions array shape (Samples per prediction, Number of Classes)
    """

    avg = np.mean(p, axis=0)
    h = -np.sum(avg * np.log(avg, where=(avg > 0)))
    return h

def normalize_global_uncertainty(h: float, num_classes: int) -> float:
    """
    Normalizes to 0.0 - 1.0 range Global uncertainty (H, Predictive entropy)

    # Parameters:
    h: Uncertainty
    num_classes: Number of prediction classes
    """

    return h / np.log(num_classes)

def expexted_entropy(p: npt.NDArray) -> float:
    """
    Calculates Expected entropy (Ep)

    # Parameters:
    p: Predictions array shape (Samples per prediction, Number of Classes)
    """

    return np.mean(-np.sum(p * np.log(p, where=(p > 0)), axis=1))


def class_predicted(p: npt.NDArray) -> int:
    """
    Returns class predicted of set of predictions (argmax of mean)

    # Parameters
    p: Predictions array shape (Samples per prediction, Number of Classes)
    """

    return np.mean(p, axis=0).argmax()


def accuracy(metrics: npt.NDArray) -> float:
    """
    Calculates prediction accuracy from its metrics

    Parameters:
    metrics: Metrics Array generated by 'analyze_predictions()'
    """

    return np.sum(metrics[:,0]) / metrics.shape[0]

def analyze_predictions(predictions: npt.NDArray, labels: npt.NDArray) -> (npt.NDArray, npt.NDArray):
    """
    Calculates acuracy of the predictions samples average and its related uncertainty metrics

    # Parameters
    predictions Array shape (Samples per prediction, Number of Predictions, Number of Classes)
    labels: Test data labels shape (Number of Predictions)

    # Returns
    metrics: Prediction related metrics shape (Numer of Predictions, 4)
        0: Correct
        1: Class Predicted
        2: H
        3: Ep
        4: Normalized H
    averages: Prediction samples average shape (Number of Predictions, Number of Classes)
    """

    num_samples, num_predictions, num_classes = predictions.shape

    metrics = []
    averages = []

    for pnum in range(num_predictions):
        p = predictions[:,pnum,:]

        avg = np.mean(p, axis=0)
        cp = avg.argmax()

        # Global uncertainty (H) -- Predictive entropy
        h = -np.sum(avg * np.log(avg, where=(avg > 0)))
        norm_h = h / np.log(num_classes)

        # Expected entropy (Ep)
        ep = np.mean(-np.sum(p * np.log(p, where=(p > 0)), axis=1))

        # correct, class predicted, H, Ep
        metrics.append([1 if cp == labels[pnum] else 0, cp, h, ep, norm_h])
        averages.append(avg.tolist())

    metrics = np.array(metrics)
    averages = np.array(averages)

    return metrics, averages


def match_ratio(a: npt.NDArray, b: npt.NDArray) -> float:
    """
    Calculates the class predicted match ratio beetwen 2 sets of predictions

    # Parameters:
    a, b: Prediction metrics arrays generated by 'analyze_predictions()'
    """

    return (np.sum(a[:,1] == b[:,1])) / a.shape[0]
