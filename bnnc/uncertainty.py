from math import log

import numpy as np
import numpy.typing as npt

def global_uncertainty(p: npt.NDArray) -> float:
    """
    Calculates Global uncertainty (H, Predictive entropy)
    
    # Parameters:
    p: Predictions array shape (Samples per prediction, Number of Classes)
    """

    avg = np.mean(p, axis=0)
    h = -np.sum(avg * np.log(avg, where=(avg > 0)))
    return h

def normalize_global_uncertainty(h: float, num_classes: int) -> float:
    """
    Normalizes to 0.0 - 1.0 range Global uncertainty (H, Predictive entropy)

    # Parameters:
    h: Uncertainty
    num_classes: Number of prediction classes
    """

    return h / np.log(num_classes)

def expexted_entropy(p: npt.NDArray) -> float:
    """
    Calculates Expected entropy (Ep)

    # Parameters:
    p: Predictions array shape (Samples per prediction, Number of Classes)
    """

    return np.mean(-np.sum(p * np.log(p, where=(p > 0)), axis=1))


def class_predicted(p: npt.NDArray) -> int:
    """
    Returns class predicted of set of predictions (argmax of mean)

    # Parameters
    p: Predictions array shape (Samples per prediction, Number of Classes)
    """

    return np.mean(p, axis=0).argmax()


def accuracy(metrics: npt.NDArray) -> float:
    """
    Calculates prediction accuracy from its metrics

    Parameters:
    metrics: Metrics Array generated by 'analyze_predictions()'
    """

    return np.sum(metrics[:,0]) / metrics.shape[0]

def calibration_errors(metrics: npt.NDArray, nbins = 10):
    bsize = 1 / nbins
    bins = np.arange(0, 1 + bsize, bsize)

    nsamples = metrics.shape[0]
    ece = 0
    uce = 0

    for i in range(nbins):
        mask = (metrics[:,5] >= bins[i]) & (metrics[:,5] < bins[i + 1])
        bin_metrics = metrics[:][mask]
        
        bin_len = bin_metrics.shape[0]
        if bin_len > 0:
            bin_correct = np.sum(bin_metrics[:,0])
            bin_err = (bin_len - bin_correct) / bin_len
            bin_uncert = np.mean(bin_metrics[:,4])
            bin_conf = np.mean(bin_metrics[:,5])
            bin_acc = np.mean(bin_metrics[:,0])

            ece += bin_len * abs(bin_acc - bin_conf) / nsamples
            uce += bin_len * abs(bin_err - bin_uncert) / nsamples

    return ece, uce

def analyze_predictions(predictions: npt.NDArray, labels: npt.NDArray) -> tuple[npt.NDArray, npt.NDArray]:
    """
    Calculates acuracy of the predictions samples average and its related uncertainty metrics

    # Parameters
    predictions Array shape (Samples per prediction, Number of Predictions, Number of Classes)
    labels: Test data labels shape (Number of Predictions)

    # Returns
    metrics: Prediction related metrics shape (Number of Predictions, 5)
        0: Correct
        1: Class Predicted
        2: H
        3: Ep
        4: Normalized H
        5: Probability
    averages: Prediction samples average shape (Number of Predictions, Number of Classes)
    """

    num_samples, num_predictions, num_classes = predictions.shape

    metrics = []
    averages = []

    for pnum in range(num_predictions):
        p = predictions[:,pnum,:]

        avg = np.mean(p, axis=0)
        cp = avg.argmax()
        prob = avg.max()

        # Global uncertainty (H) -- Predictive entropy
        h = -np.sum(avg * np.log(avg, where=(avg > 0)))
        norm_h = normalize_global_uncertainty(h, num_classes)

        # Expected entropy (Ep)
        ep = np.mean(-np.sum(p * np.log(p, where=(p > 0)), axis=1))

        # correct, class predicted, H, Ep
        metrics.append([1 if cp == labels[pnum] else 0, cp, h, ep, norm_h, prob])
        averages.append(avg.tolist())

    metrics = np.array(metrics)
    averages = np.array(averages)

    return metrics, averages


def match_ratio(a: npt.NDArray, b: npt.NDArray) -> float:
    """
    Calculates the class predicted match ratio beetwen 2 sets of predictions

    # Parameters:
    a, b: Prediction metrics arrays generated by 'analyze_predictions()'
    """

    match_mask = a[:,1] == b[:,1]
    diff_mask = ~match_mask

    return np.sum(match_mask) / a.shape[0], diff_mask
